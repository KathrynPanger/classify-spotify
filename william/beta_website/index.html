<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Genre Classification using Machine Learning</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">


  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
  <link href="assets/css/prism.css" rel="stylesheet">
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top ">
    <div class="container-fluid">

      <div class="row justify-content-center">
        <div class="col-xl-9 d-flex align-items-center justify-content-between">
          <!-- <h1 class="logo"><a href="index.html">Machine Learning<br>Spotify Genres</a></h1> -->
          <a href="index.html" class="logo"><img src="assets/img/logo.png" alt="" class="img-fluid"></a>

          <nav class="nav-menu d-none d-lg-block">
            <ul>
              <li class="active"><a href="index.html">Home</a></li>
              <li><a href="#brief">Brief</a></li>
              <li><a href="#spotify">Spotify API Call</a></li>
              <li><a href="#data_transformation">Data Transformation</a></li>
              <li><a href="#data-pre-processing">Data Generation with Librosa</a></li>
              <li><a href="#machine-learning">Machine Learning</a></li>
              <li><a href="#contact">The Team</a></li>

            </ul>
          </nav><!-- .nav-menu -->

          <a href="https://github.com/KathrynPanger/classify-spotify" target="_blank" class="get-started-btn scrollto">Github Repo</a>
        </div>
      </div>

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero" class="d-flex flex-column justify-content-center">
    <div class="container">
      <div class="row justify-content-center">
        <div class="typewriter">
          <h1>Machine Learning: Genre Classification</h1>
          <h2>A project to build a 'Trained Classifier' to identify a song's genre</h2>
          <!-- <a href="https://www.youtube.com/watch?v=571BuZeeQjE" class="venobox play-btn mb-4" data-vbtype="video" data-autoplay="true"></a> -->
        </div>
      </div>
    </div>
  </section><!-- End Hero -->

  <main id="main">

    <!-- ======= About Us Section ======= -->
    <section id="brief" class="about">
      <div class="container">

        <div class="section-title">
          <h2>Brief</h2>
          <p>We all know that musical genres are a way to tag and identify different types of music. But what most people don't know, is that basic genres have key defining features, that help us to identify them - and so the team's project idea was born...<br><br>The team's objective
            was to use machine learning to build its very own 'Trained Classifier', that would use machine learning to identify a given song’s musical genre. The musical genres in-scope for this project were: <b>Reggae Rock,
            Jazz, Pop, Blues, Classical, Country, Disco, Hip-Hop, Heavy Metal </b> </p>
        </div>

        <div class="row content">
          <div class="col-lg-6">
            <p>
              The team broke down the process of creating and training the Trained Classifier into the following high-level steps:<br><br>
            </p>
            <ul>
              <li><i class="ri-spotify-fill"></i> <b>Spotify API Call</b> - utilizing the 'spotipy' library to call Spotify's API in order to pull track information data (inc. URLs to audio clips) for the musical genres in-scope.<br><br><br></li>
              <li><i class="ri-merge-cells-horizontal"></i> <b>Data Transformation</b> - transforming the API collected data, prior to data pre-processing.<br><br><br></li>
              <li><i class="ri-voiceprint-fill"></i> <b>Pre-processing of data using Librosa</b> - downloading and extracting each audio clip's 6 key audio class features, before saving in a DataFrame, ready for machine learning.</b><br><br><br> </li>
              <li><i class="ri-eye-2-line"></i> <b>Machine learning using sklearn</b> - harnessing the power of sklearn to train our Trained Classifier.</b><br><br><br> </li>
              <li><i class="ri-bug-line"></i> <b>Testing</b> - finally a testing regime will let us analize weather the Trained Classifier.</b><br><br></li>
            </ul>
          </div>
          <div class="image col-lg-6 pt-4 pt-lg-0" style='background-image: url("assets/img/flow_chart.gif");'></div>

    </section><!-- End About Us Section -->

    <!-- ======= Dependencies Section ======= -->
    <section id="clients" class="clients">
      <div class="container">
        <h4><b>Our dependencies</b></h4>

        <div class="row no-gutters clients-wrap clearfix wow fadeInUp">

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-1.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-2.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-3.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-4.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-5.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-6.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-7.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-8.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-blank.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-10.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-9.png" class="img-fluid" alt="">
            </div>
          </div>

          <div class="col-lg-3 col-md-4 col-xs-6">
            <div class="client-logo">
              <img src="assets/img/clients/client-blank.png" class="img-fluid" alt="">
            </div>
          </div>


        </div>

      </div>
    </section><!-- End Dependencies Section -->


    <!-- ======= Cta Section ======= -->
    <section id="cta" class="cta">
          <div class="container">

            <div class="row">
              <div class="col-lg-9 text-center text-lg-left">
                <h3>Spotify facts...</h3>
                <p>• <b><span class="spotify_green">Founded in 2008</span></b>, but didn't launch until 2011<br>
                • Has over <b><span class="spotify_green">286 million monthly active users</span></b> as of April 2020.<br>
                • Spotify’s <b><span class="spotify_green">free accounts were once available through invitations</span></b> only to control its growth.
              </p>
              </div>
              <div class="col-lg-3 cta-btn-container text-center">
                <a class="cta-btn align-middle" href="https://developer.spotify.com/documentation/web-api/" target="_blank">Visit Spotify's API</a>
              </div>
            </div>

          </div>
        </section>

    <!-- End Cta Section -->

    <!-- ======= Spotify API Section ======= -->
    <section id="spotify" class="features">
      <div class="container">
        <div class="section-title">


          <h2>Spotify API Call</h2>
          <p>With a track catalog spanning in excess of 30 million songs, the team quickly reached
            concensus that Spotify's API provided the most robust and well curated universe of track information and
            was chosen. Additionally, Spotify also provides 30-sec samples for each cataloged song, thus providing the
            team with a source of track mp3 samples to use for trainined the Trained Classifier. Each team member chosen
            2 musical genres of personal interest, before completing steps to call Spotify's API -
          </p>
        </div>

        <div class="row">
          <div class="col-lg-13 order-2 order-lg-1">
            <div class="icon-box mt-5 mt-lg-0">
              <i class="bx bx-slider-alt"></i>

              <h4>Step 1 - setup</h4>
              <p>The required dependencies were imported including:<br><br><b>(i) Spotipy</b> - a lightweight Python library for
                accessing the Spotify Web API.<br><br><b>(ii) Pandas</b> - utilized for turning returned API Call
                data into DataFrames, prior to data transformation.</p>
              <pre>
                <code class="language-python">
                  import spotipy
                  from spotipy.oauth2 import SpotifyClientCredentials
                  import pandas as pd
                </code>
              </pre>
            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-data"></i>
              <h4>Step 2 - API call</h4>
              <p>By using the 'recommendation' parameter, the team were able to return track information for a chosen genre,
              whilst keeping the return limit to 100, in order to avoid reaching the Spotify API call limits and avoid timeouts.</p>
              <pre>
                <code class="language-python">
                  from config import cid
                  from config import secret
                  client_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)
                  sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

                  randoms = sp.recommendations(seed_genres=['rock'], limit=100)
                </code>
              </pre>

            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-list-plus"></i>
              <h4>Step 3 - storage & verification of returned data</h4>
              <p>Using Pandas, each track element was saved to a namesake list, prior to relevant track information
              being written to a .csv file, for reference purposes.</p>
              <pre>
                <code class="language-python">
                  tracks_preview = []
                  tracks_genre = []
                  tracks_name = []
                  tracks_id = []
                  tracks_artist = []
                  for album_ind in range(0, 100):
                      tracks_genre.append(randoms['seeds'][0]['id'])
                      tracks_name.append(randoms['tracks'][album_ind]['name'])
                      tracks_id.append(randoms['tracks'][album_ind]['id'])
                      tracks_preview.append(randoms['tracks'][album_ind]['preview_url'])
                </code>
              </pre>
            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-import"></i>
              <h4>Step 4 - preparing for data transformation</h4>
              <p>The .csv files were then imported as DataFrames to Jupyter Lab, ready for data transformation. </p>
              <pre>
                <code class="language-python">
                  df5 = pd.DataFrame({'Genre':tracks_genre,'Track Name' : tracks_name, 'Track ID' : tracks_id, 'Preview_link' : tracks_preview})
                  df5.to_csv("rock/rock_dataframe_5.csv", index=False)                </code>
              </pre>
            </div>
          </div>
          <!-- <div class="image col-lg-6 order-1 order-lg-2" style='background-image: url("assets/img/api_code_example.gif");'></div> -->
        </div>
      </div>
    </section><!-- End Spotify API Section -->

    <!-- ======= Spotify API Section ======= -->
    <section id="counts" class="counts">
      <div class="container">

        <div class="text-center title">
          <h3>Spotify API Stats</h3>
          <p>Here's a snapshot of the data the team captured, during the API Calls...</p>
        </div>

        <div class="row counters">

          <div class="col-lg-3 col-6 text-center">
            <span data-toggle="counter-up">6</span>
            <p>Team members</p>
          </div>

          <div class="col-lg-3 col-6 text-center">
            <span data-toggle="counter-up">1415</span>
            <p>Tracks</p>
          </div>

          <div class="col-lg-3 col-6 text-center">
            <span data-toggle="counter-up">5094</span>
            <p>Playback (hrs)</p>
          </div>

          <div class="col-lg-3 col-6 text-center">
            <span data-toggle="counter-up">132,000</span>
            <p>Collected Audio Class Features</p>
          </div>
        </div>

      </div>
    </section><!-- End Spotify API Section -->

    <!-- ======= Data Transformation Section ======= -->
    <section id="data_transformation" class="data_transformation">
      <div class="container">
        <div class="section-title">
          <h2>Data Transformation</h2>
          <p>Prior to the pre-processing of track data, for the purpose of machine learning, some basic
             data transformation was performed, to ensure data integrity and accuracy:<br><br></p>

        <div class="row">
          <div class="col-lg-6 order-2 order-lg-1">
            <div class="icon-box mt-5 mt-lg-0">
              <i class="bx bx-layer-minus"></i>
              <h4>Dropping duplicates</h4>
              <p>Some audio tracks appear in Spotify's catalog multiple times for various valid reasons,
              these duplicates were dropped from their respective dataframes.</p>
            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-list-minus"></i>
              <h4>Purging obsolete data columns</h4>
              <p>Whilst all API collected data served a surpose for referencing purpose, these now obsolete
              data columns were purged.</p>
            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-fullscreen"></i>
              <h4>Removing white space</h4>
              <p>Datapoints, such as 'track names' and 'preview URLs' contained white space, that required
              to be removed.</p>
            </div>
            <div class="icon-box mt-5">
              <i class="bx bx-horizontal-center"></i></i>
              <h4>Dataframe Joins</h4>
              <p>Once data transformation was completed, some dataframes      were merged, in preparation
              for Data Pre-processing.</p>
            </div>
          </div>
          <div class="image col-lg-6 order-1 order-lg-2" style='background-image: url("assets/img/code_shot_3.jpg");'></div>
        </div>

      </div>
    </section><!-- End Data Transformation Section -->

    <!-- ======= Cta Section ======= -->
    <section id="cta1" class="cta1">
          <div class="container">

            <div class="row">
              <div class="col-lg-9 text-center text-lg-left">
                <h3>Librosa Facts</h3>
                <p> <code class="language-python">librosa</code> is a python package for music and audio analysis.<br>
                  It provides the building blocks necessary to create music information retrieval systems."</p>
              </div>
              <div class="col-lg-3 cta-btn-container text-center">
                <a class="cta-btn align-middle" href="https://librosa.org/doc/latest/index.html" target="blank">Learn more</a>
              </div>
            </div>
          </div>
        </section>
    <!-- End Cta Section -->

    <!-- ======= Data Generation with Librosa Section ======= -->
    <section id="data-pre-processing" class="data-pre-processing">
      <div class="container-fluid">

        <div class="row">

          <div class="col-lg-7 d-flex flex-column justify-content-center align-items-stretch  order-2 order-lg-1">

            <div class="content">
              <h3><strong>Data Generation with Librosa</strong></h3>
              <p>
                Having called the Spotify API, returned data (including URLs that, in turn, gave us the ability to download Spotify's
                30-sec MP3 audio clips for each song in our chosen genres), the next stage was to generate the data that would subsequently
                be used in machine learning, to train the Trained Classifier.<br><br>Specifically, the Audio Class Features would be extracted from the MP3 audio
                clips, appended to a dictionary of lists, before being saved to both a pandas DataFrame (<code class="language-python">pd.DataFrame(basedata)</code>)
                 and written to .csv files (<code class="language-python">base_df.co_csv</code>)
              </p>
            </div>

            <div class="accordion-list">
              <ul>
                <li>
                  <a data-toggle="collapse" class="collapse" href="#accordion-list-1">1. Setup & Importing Dependencies<i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-1" class="collapse" data-parent=".accordion-list">
                    <p>
                      As the team prepared for data pre-processing, Juypter Lab (the previously used IDE) was quickly identified as having  limitations (namely the distinct lack of
                      collaborative functionality), so instead the team efficiently migrated to Google Colab &#127881; and <code class="language-python">pip installed</code> and imported our dependencies.
                      <pre>
                        <code class="language-python">
                          import pandas as pd
                          import librosa
                          from librosa.core import istft
                          import librosa.display
                          import IPython.display as ipd
                          import skimage
                          from skimage import io
                          import numpy as np
                          import matplotlib.pyplot as plt
                          import sklearn
                       </code>
                     </pre>

                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" href="#accordion-list-2" class="collapsed">2. Creating a base Dictionary & MFCC List <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-2" class="collapse" data-parent=".accordion-list">
                    <p>
                      Before data could be prepared and pre-processed for machine learning, a dictionary <code class="language-python">basedata</code>
                      was required to initially house all extracted Audio Features, and eventually the <b>Mel-frequency cepstral coefficients</b>, although
                      we prefer calling them <b>‘MFCC’</b> personally! &#128521; -

                      <pre>
                        <code class="language-python">
                          # Creating our base DataFrame with column heading.
                          # Note: the team found that saving to a DataFrame as this stage caused values to
                          # be cut-off, so defered this transition to a DataFrame, until later in the process
                          basedata = {"Genre":[],
                                     "Chroma Feature":[],
                                     "Zero-Crossing Rate":[],
                                     "Spectral Bandwidth":[],
                                     "Root Mean Square Energy (RSME)":[],
                                     "Spectral Rolloff":[],
                                     "Spectral Centroid":[]
                                     }
                          # Creating a list for each mfcc value in dictionary
                          for i in range(0,15):
                            mfcc_title = f"mfcc({i})"
                            mfcc_value = []
                            basedata[mfcc_title] = mfcc_value
                        </code>
                      </pre>
                      <b>Why didn't the team create a pandas DataFrame?</b> Through trial-and-error,
                       the team identified that certain values would be cut off, so defered the transition to
                       a DataFrame until later in the process
                    </p>
                  </div>
                </li>

                <li>
                  <a data-toggle="collapse" href="#accordion-list-3" class="collapsed">3. Spectrogram setup <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-3" class="collapse" data-parent=".accordion-list">
                    <p>
                      With the <code class="language-python">basedata</code> dictionary of lists created to store the Audio Class Features for each MP3, the next step was to extract the Audio Class Features, by means of a Spectrogram.
                      <br><br> A <code class="language-python">create_spec</code> function was created to take an MP3 file and render a Spectrogram, before the function would harness built-in Librosa functions to extract the following
                      Audio Class Features: <br><br><span class="greenText">(&#10004;)</span> Spectral Centroid
                      <br><span class="greenText">(&#10004;)</span> Spectral Roll-off <br><span class="greenText">(&#10004;)</span> Spectral Bandwidth <br><span class="greenText">(&#10004;)</span> Chroma Feature<br> <span class="greenText">(&#10004;)</span> Root-mean-square deviation (RMSD) <br><span class="greenText">(&#10004;)</span> Mel-frequency cepstrum coefficients (MFCC)</b>.
                      <br><br>
                      The first task was to create a function <code class="language-python">scale_minmax</code> that defines the scale of the spectrogram, required to extract Audio Class Features for each MP3 clip - <br>
                        <pre>
                          <code class="language-python">
                            def scale_minmax(X, min=0.0, max=1.0):
                                X_std = (X - X.min()) / (X.max() - X.min())
                                X_scaled = X_std * (max - min) + min
                                return X_scaled
                            </code>
                          </pre>
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" href="#accordion-list-4" class="collapsed">4a. Audio Class Feature extraction - Main Function Setup <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-4" class="collapse" data-parent=".accordion-list">
                    <p>
                      Once <code class="language-python">scale-minmax</code> had setup and calibrated the Spectrogram, the main <code class="language-python">create_spec</code> function was then
                      coded, with the purpose of using Librosa built-in functions to compute the Audio Class Features. The following shows the boilerplate setup code for the function - <br>
                      <pre>
                        <code class="language-python">
                          # Defining the main function that passes 'file' variable as an arguement, that
                          # will be introduce the MP3 file to the function
                          def create_spec(file, g):
                          x , sr = librosa.load(file)
                          global basedata
                        </code>
                      </pre>

                    </p>
                  </div>
                </li>

                <li>
                  <a data-toggle="collapse" href="#accordion-list-5" class="collapsed">4b. Audio Class Feature extraction - Capturing the Audio Class Features (ex. mfcc) <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-5" class="collapse" data-parent=".accordion-list">
                    <p>
                      In this part of the <code class="language-python">create_spec</code> function, Librosa's built-in functions are utilized to capture the Audio Class Features for the MP3 file - <br>
                      <pre>
                        <code class="language-python">
                          # Finding the Spectral Centroid value aka
                          # Definition: "Center of mass sound"
                          spectral_centroids = librosa.feature.spectral_centroid(x,  sr=sr)

                          # Computing Root Mean Square Energy
                          # Definition: the square root of the mean square (the arithmetic mean of
                          the squares of a set of numbers)
                          rmse = librosa.feature.rmse(x)

                          # Finding the Spectral Roll-off value
                          # Definition: measures how often high frequencies (peaks) reduce to zero
                          spectral_rolloff = librosa.feature.spectral_rolloff(x, sr=sr)

                          # Finding the Spectral Bandwidth value
                          # Definition: the width the of band of height of one-half the peal maximum
                          spectral_bandwidth = librosa.feature.spectral_bandwidth(x, sr=sr)

                          # Finding the Zero Crossing Rate value
                          # Definition: calculates the smoothness of a signal, typically higher in rock/metal
                          zcr = librosa.feature.zero_crossing_rate(x)

                          # Finding Chroma feature value
                          # Definition: how much energy of each pitch class is present
                          hromagram = librosa.feature.chroma_stft(x, sr=sr)
                        </code>
                      </pre>

                    </p>
                  </div>

                </li>

                <li>
                  <a data-toggle="collapse" href="#accordion-list-9" class="collapsed">4c. Audio Class Feature extraction - Capturing MFCC Audio Class Feature <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-9" class="collapse" data-parent=".accordion-list">
                    <p>
                      Unlike the other Audio Class Features, mfcc was computed using different settings for each dataset. It was for this reason, that this part of the codeset was isolated to avoid confusion
                      and to ensure a totally modular codeset.<br><br>
                      The 4 setting groups used (based on the dataset) are as follows:<br>
                      <span class="greenText">(&#10004;)</span> Break song into <b>30</b> equal segments, each <b>01</b> second long, and loop through<br>
                      <span class="greenText">(&#10004;)</span> Break song into <b>10</b> equal segments, each <b>03</b> second long, and loop through<br>
                      <span class="greenText">(&#10004;)</span> Break song into <b>15</b> equal segments, each <b>02</b> second long, and loop through<br>
                      <span class="greenText">(&#10004;)</span> Break song into <b>05</b> equal segments, each <b>06</b> second long, and loop through<br><br>

                      <b><u>Useful definitions:</u></b><br>
                        <span class="greenText">&#10067;</span>- <code class="language-python">sps</code> (Samples per segment) - refers to samples per track, sliced into equal parts<br>
                        <span class="greenText">&#10067;</span>- <code class="language-python">sr</code> (Sample rate) - literally refers to how fast samples are table (measured in <code class="language-python">sps</code>)<br><br>


                      <b>Please Note:</b> the below code is indicative of the MP3 audio file is 30-seconds in length and the code is slicing the file into 15 equal parts.
                      Each equal part is two seconds long, so in this case the <code class="language-python">sps=sr*2</code>

                      <pre>
                        <code class="language-python">
                          for i in range(0,15):
                            # Settings are different for each mfcc so we nested settings
                            # within the loop
                            sps=sr*2
                            start_sample=sps*i
                            window = x[(start_sample):(start_sample+sr)]
                            X = librosa.stft(window)
                            Xdb = librosa.amplitude_to_db(abs(X))
                            Xdb = istft(Xdb)

                            #find mffc (Mel frequency cepstral coefficients)
                            #Definition: the measure in peaks of sound in a given segment
                            mfcc = librosa.feature.mfcc(window, sr)
                            mfcc = mfcc.T

                            # Appending mfcc values to lists
                            mfcc_title = f"mfcc({i})"
                            mfcc_value = [mfcc[i]]
                            basedata[mfcc_title].append(np.mean(mfcc_value))
                        </code>
                      </pre>

                    </p>
                  </div>

                </li>


                <li>
                  <a data-toggle="collapse" href="#accordion-list-8" class="collapsed">4d. Audio Class Feature extraction - Appending Audio Class Features <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list-8" class="collapse" data-parent=".accordion-list">
                    <p>
                      Once <code class="language-python">create_spec</code> has captured all the Audio Class Features (mfcc is purposely exluded for now), the features are appended to the <code class="language-python">basedata</code> dictionary of lists -
                      <pre>
                        <code class="language-python">
                          # Notice that the mean values of each Audio Class Feature are appended to it's
                          # respective list in basedata
                          basedata["Genre"].append(f'{g}')
                          basedata["Chroma Feature"].append(np.mean([chromagram]))
                          basedata["Zero-Crossing Rate"].append(np.mean([zcr]))
                          basedata["Spectral Bandwidth"].append(np.mean([spectral_bandwidth]))
                          basedata["Root Mean Square Energy (RSME)"].append(np.mean([rmse]))
                          basedata["Spectral Rolloff"].append(np.mean([spectral_rolloff]))
                          basedata["Spectral Centroid"].append(np.mean([spectral_centroids]))
                        </code>
                      </pre>

                    </p>
                  </div>

                </li>


                <li>
                  <a data-toggle="collapse" href="#accordion-list6" class="collapsed">5. Handling the MP3 files in the <code class="language-python">create_spec</code> function  <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list6" class="collapse" data-parent=".accordion-list1">
                    <p>
                      Whilst the main <code class="language-python">create_spec</code> function handles the extraction of Audio Class Features from the MP3 files, it relies on a dependency named <code class="language-python">glob</code>.
                      Once pointed to a path, <code class="language-python">glob</code> purpose is to find all files in the directory that meet a user-defined syntax, in this case MP3 files.<br><br>
                      It's purpose in this case is to iterate through and save the MP3 data in a variable, which is then passed as an argument to the main <code class="language-python">create_spec</code> function - <br>
                      <pre>
                        <code class="language-python">
                          import glob
                          import pathlib
                          import os

                          # This cell loops through all mp3s in all genre folders and applies the create spec
                          # function and produces a compiled dataframe at the end with 1000 rows
                          genre_list= ["blues","classical","country","disco","h_metal","hiphop","jazz","pop",
                          "reggae","rock"]

                          for g in genre_list:

                            pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)
                            for filename in os.listdir(f'./drive/My Drive/FinalDataProject/{g}'):
                                songname = f'./drive/My Drive/FinalDataProject/{g}/{filename}'
                                create_spec(songname, g)
                        </code>
                      </pre>
                    </p>
                  </div>
                </li>

                <li>
                  <a data-toggle="collapse" href="#accordion-list7" class="collapsed">6. Saving Audio Class Features To .CSV & DataFrame <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list7" class="collapse" data-parent=".accordion-list1">
                    <p>
                      Once the <code class="language-python">create_spec</code> function had extracted and appended all the Audio Features to the respective list
                      within the <code class="language-python">basedata</code> dictionary, it was time to save all the data appropriately:<br><br>
                      <span class="greenText">(&#10004;)</span> the <code class="language-python">basedata</code> was converted to a DataFrame - <br>
                      <pre>
                        <code class="language-python">
                          base_df = pd.DataFrame(basedata)
                        </code>
                      </pre>
                      <br><span class="greenText">(&#10004;)</span> Finally, the <code class="language-python">base_df</code> was written to a CSV file<br>
                      <pre>
                        <code class="language-python">
                          base_df.to_csv("drive/My Drive/FinalDataProject/Machine/numerical_machine_15.csv", index=False)
                        </code>
                      </pre>
                    </p>
                  </div>
                </li>

              </ul>
            </div>

          </div>

          <div class="col-lg-5 align-items-stretch order-1 order-lg-2 img" style='background-image: url("assets/img/data-pre-processing.jpg");'>&nbsp;</div>
        </div>

      </div>
    </section><!-- End Data Generation with Librosa Section Section -->

    <!-- ======= Cta Section ======= -->
    <section id="cta2" class="cta2">
          <div class="container">

            <div class="row">
              <div class="col-lg-9 text-center text-lg-left">
                <h3>Sklearn Fact Check</h3>
                <p> <code class="language-python">Scikit-learn</code>is a free software machine learning library for the Python programming language.<br>
                  It features various classification, regression and clustering algorithms including support vector machines</p>
              </div>
              <div class="col-lg-3 cta-btn-container text-center">
                <a class="cta-btn align-middle" href="https://scikit-learn.org/stable/">Visit Sklearn</a>
              </div>
            </div>
          </div>
        </section>
    <!-- End Cta Section -->

    <!-- ======= Machine Learning ======= -->
    <section id="machine-learning" class="machine-learning">
      <div class="container-fluid">

        <div class="row">

          <div class="col-lg-7 d-flex flex-column justify-content-center align-items-stretch  order-2 order-lg-1">

            <div class="content">
              <h3><strong>Creating a Deep Learning Model</strong></h3>
              <p>
                After generating multiple clean datasets with a varying number of features/mfccs using Librosa, we had one final task: use that data to construct a deep learning model.<br><br>
                The final step consisted of splitting the datasets into train/test data, creating a neural network and running tests on it to determine the optimum number of layers, nodes, and epochs.
              </p>
            </div>

            <div class="accordion-list1">
              <ul>
                <li>
                  <a data-toggle="collapse" class="collapse" href="#accordion-list1">1. Setup & Importing Dependencies<i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list1" class="collapse" data-parent=".accordion-list1">
                    <p>
                      As the team decided to migrate from Jupyter Lab to Google Colab, the setup of the now, Colab file and respective depedencies changed slightly.<br><br>
                      Tensorflow was chosen as the end- machine learning framework to utilize, whilst <code class="language-python">numpy</code> and
                       <code class="language-python">pandas</code> were used for data visulization and manipulation - <br>
                      <pre>
                        <code class="language-python">
                          # Importing the users drive, in order to access and write files pertinent to the
                          # machine learning code file.
                          x`from google.colab import drive
                          drive.mount('/content/drive')

                          # Importing dependencies for data manipulation and visualization
                          import numpy as np
                          import pandas as pd

                          # Finally importing the machine learning framework, to be utilized for
                          # the Trained Classifier
                          import tensorflow
                          import sklearn
                          tensorflow.keras.__version__
                        </code>
                      </pre>
                      <b>Note:</b> whilst the above make up all the required dependencies, modules from <code class="language-python">tensorflow</code>
                       and <code class="language-python">sklearn</code> have been omitted from the above, instead being referenced appropriately, above
                       from the code they are required for in later steps.
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" href="#accordion-list2" class="collapsed">2. Data Pre-processing <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list2" class="collapse" data-parent=".accordion-list1">
                    <p>
                      <span class="greenText">(&#10004;)</span> The team began by defining the dependent variable (y) and all the independent variables (x) before printing for verification - <br>
                      <pre>
                        <code class="language-python">
                          X = genre.drop("Genre", axis=1)
                          y = genre["Genre"]
                          print(X.shape, y.shape)
                        </code>
                      </pre>
                      <br> <span class="greenText">(&#10004;)</span> Having then imported sklearn and tensor flow dependencies, the data was shuffled and split 80% train
                      and 20% test data, using sklearn - <br>
                      <pre>
                        <code class="language-python">
                          from sklearn.model_selection import train_test_split
                          from sklearn.preprocessing import LabelEncoder, MinMaxScaler
                          from tensorflow.keras.utils import to_categorical

                          X_train, X_test, y_train, y_test = train_test_split(
                              X, y, random_state=1)
                          </code>
                        </pre>
                      <br> <span class="greenText">(&#10004;)</span> X data was then scaled to fit the model extents, mostly for performance and efficiency. In layman's terms,
                       the machine is zooming into the active portion of the graph and ignoring the blank, inactive parts -  <br>
                       <pre>
                         <code class="language-python">
                           X_scaler = MinMaxScaler().fit(X_train)
                           X_train_scaled = X_scaler.transform(X_train)
                           X_test_scaled = X_scaler.transform(X_test)
                         </code>
                       </pre>
                      <br> <span class="purpleText">(&#10004;)</span> <b>Part (a) -</b> The genre string was then label-encoded into integer, using <code class="language-python">sklearn</code><br>
                      <pre>
                        <code class="language-python">
                          label_encoder = LabelEncoder()
                          label_encoder.fit(y_train)
                          encoded_y_train = label_encoder.transform(y_train)
                          encoded_y_test = label_encoder.transform(y_test)
                        </code>
                      </pre>
                      <br> <span class="purpleText">(&#10004;)</span> <b>Part (b) -</b> <code class="language-python">keras</code> was then utilized to convert encoded labels to one-hot-encoding (integers to binary code - <br>
                      <pre>
                        <code class="language-python">
                          y_train_categorical = to_categorical(encoded_y_train)
                          y_test_categorical = to_categorical(encoded_y_test)
                        </code>
                      </pre>
                      <br> <span class="greenText">(&#10004;)</span> At this stage, the deep learning dependencies were imported, before testing 3 datasets with 3 different inputs. <b>The purpose
                      of this particular test is testing the accuracy rating for the trainging data.</b>
                       E.g - in code example below: <br><br>
                      <span class="purpleText">(&#9642;)</span> <b>21 input dimensions</b> (one for each feature and MFCC), with the <b>output set to 10</b>, in-line with the number of genres in scope.<br>
                      <span class="purpleText">(&#9642;)</span> <b>85 nodes</b> are present in the <b>input layer</b>, with <b>75 nodes in the first hidden layer</b>, and <b>10 nodes</b> in the second hidden layer - <br>
                      <pre>
                        <code class="language-python">
                          # The best score in this example dataset was c.51% accuracy.
                          from tensorflow.keras.models import Sequential
                          from tensorflow.keras.layers import Dense
                          from keras.models import Sequential, load_model
                          from keras.layers.core import Dense, Dropout, Activation
                          from keras.utils import np_utils

                          model = Sequential()
                          model.add(Dense(units=85, activation='relu', input_dim=21))
                          model.add(Dense(units=75, activation='relu'))
                          model.add(Dense(units=10, activation='relu'))
                          model.add(Dense(units=10, activation='softmax'))
                        </code>
                      </pre>
                      <br> <span class="greenText">(&#10004;)</span> The model was then compiled and fitted, before displaying it's shape / summary - <br>
                      <pre>
                        <code class="language-python">
                          model.compile(optimizer='adam',
                                        loss='categorical_crossentropy',
                                        metrics=['accuracy'])
                          model.summary()
                        </code>
                      </pre>
                      ______________________________________________________________________________________________
                      <table style="width:100%">
                        <tr>
                          <th>Layer (type)</th>
                          <th>Output Shape</th>
                          <th>Param #</th>
                        </tr>
                        <tr>
                          <td>dense (Dense)</td>
                          <td>(None, 85)</td>
                          <td>1870</td>
                        </tr>
                        <tr>
                          <td>dense_1 (Dense)</td>
                          <td>(None, 75)</td>
                          <td>6450</td>
                        </tr>
                        <tr>
                          <td>dense_2 (Dense)</td>
                          <td>(None, 10)</td>
                          <td>760</td>
                        </tr>
                        <tr>
                          <td>dense_3 (Dense)</td>
                          <td>(None, 10)</td>
                          <td>110</td>
                        </tr>
                      </table>
                      ______________________________________________________________________________________________<br>
                      <br> <span class="greenText">(&#10004;)</span> Before the team could quantify the model, the model was trained for 120-cycles and tested against test data - <br>
                      <pre>
                        <code class="language-python">
                      model.fit(
                          X_train_scaled,
                          y_train_categorical,
                          epochs=120,
                          shuffle=True,
                          verbose=2
                      )

                      model_loss, model_accuracy = model.evaluate(
                          X_test_scaled, y_test_categorical, verbose=2)
                      print(
                          f"Normal Neural Network - Loss:{model_loss},Accuracy:{model_accuracy}")
                        </code>
                      </pre>
                      <br> <span class="greenText">&#128221;</span> <b>Results: </b> Here are the testing results of the model, against training data: <br>
                      <div style="height:120px;width:765px;border:1px solid #ccc;font:16px/26px Georgia, Garamond, Serif;overflow:auto;">
                        Epoch 1/120<br>
                        24/24 - 0s - loss: 2.2899 - accuracy: 0.1107<br>
                        Epoch 2/120<br>
                        24/24 - 0s - loss: 2.2741 - accuracy: 0.1320<br>
                        Epoch 3/120<br>
                        24/24 - 0s - loss: 2.2506 - accuracy: 0.1547<br>
                        Epoch 4/120<br>
                        24/24 - 0s - loss: 2.2199 - accuracy: 0.1933<br>
                        Epoch 5/120<br>
                        24/24 - 0s - loss: 2.1914 - accuracy: 0.2147<br>
                        Epoch 6/120<br>
                        24/24 - 0s - loss: 2.1507 - accuracy: 0.2453<br>
                        Epoch 7/120<br>
                        24/24 - 0s - loss: 2.1055 - accuracy: 0.2653<br>
                        Epoch 8/120<br>
                        24/24 - 0s - loss: 2.0576 - accuracy: 0.2773<br>
                        Epoch 9/120<br>
                        24/24 - 0s - loss: 2.0049 - accuracy: 0.2827<br>
                        Epoch 10/120<br>
                        24/24 - 0s - loss: 1.9475 - accuracy: 0.2880<br>
                        Epoch 11/120<br>
                        24/24 - 0s - loss: 1.8831 - accuracy: 0.3027<br>
                        Epoch 12/120<br>
                        24/24 - 0s - loss: 1.8403 - accuracy: 0.3133<br>
                        Epoch 13/120<br>
                        24/24 - 0s - loss: 1.7768 - accuracy: 0.3120<br>
                        Epoch 14/120<br>
                        24/24 - 0s - loss: 1.7659 - accuracy: 0.3320<br>
                        Epoch 15/120<br>
                        24/24 - 0s - loss: 1.7240 - accuracy: 0.3467<br>
                        Epoch 16/120<br>
                        24/24 - 0s - loss: 1.6810 - accuracy: 0.3507<br>
                        Epoch 17/120<br>
                        24/24 - 0s - loss: 1.6872 - accuracy: 0.3440<br>
                        Epoch 18/120<br>
                        24/24 - 0s - loss: 1.6335 - accuracy: 0.3760<br>
                        Epoch 19/120<br>
                        24/24 - 0s - loss: 1.6117 - accuracy: 0.4013<br>
                        Epoch 20/120<br>
                        24/24 - 0s - loss: 1.6136 - accuracy: 0.4000<br>
                        Epoch 21/120<br>
                        24/24 - 0s - loss: 1.5995 - accuracy: 0.4013<br>
                        Epoch 22/120<br>
                        24/24 - 0s - loss: 1.5761 - accuracy: 0.4133<br>
                        Epoch 23/120<br>
                        24/24 - 0s - loss: 1.5785 - accuracy: 0.3840<br>
                        Epoch 24/120<br>
                        24/24 - 0s - loss: 1.5388 - accuracy: 0.4200<br>
                        Epoch 25/120<br>
                        24/24 - 0s - loss: 1.5280 - accuracy: 0.4413<br>
                        Epoch 26/120<br>
                        24/24 - 0s - loss: 1.5329 - accuracy: 0.4387<br>
                        Epoch 27/120<br>
                        24/24 - 0s - loss: 1.5298 - accuracy: 0.4173<br>
                        Epoch 28/120<br>
                        24/24 - 0s - loss: 1.5068 - accuracy: 0.4307<br>
                        Epoch 29/120<br>
                        24/24 - 0s - loss: 1.4958 - accuracy: 0.4360<br>
                        Epoch 30/120<br>
                        24/24 - 0s - loss: 1.4943 - accuracy: 0.4387<br>
                        Epoch 31/120<br>
                        24/24 - 0s - loss: 1.4711 - accuracy: 0.4533<br>
                        Epoch 32/120<br>
                        24/24 - 0s - loss: 1.4992 - accuracy: 0.4133<br>
                        Epoch 33/120<br>
                        24/24 - 0s - loss: 1.4670 - accuracy: 0.4440<br>
                        Epoch 34/120<br>
                        24/24 - 0s - loss: 1.4613 - accuracy: 0.4467<br>
                        Epoch 35/120<br>
                        24/24 - 0s - loss: 1.4459 - accuracy: 0.4547<br>
                        Epoch 36/120<br>
                        24/24 - 0s - loss: 1.4259 - accuracy: 0.4560<br>
                        Epoch 37/120<br>
                        24/24 - 0s - loss: 1.4362 - accuracy: 0.4560<br>
                        Epoch 38/120<br>
                        24/24 - 0s - loss: 1.4282 - accuracy: 0.4600<br>
                        Epoch 39/120<br>
                        24/24 - 0s - loss: 1.4228 - accuracy: 0.4520<br>
                        Epoch 40/120<br>
                        24/24 - 0s - loss: 1.4103 - accuracy: 0.4560<br>
                        Epoch 41/120<br>
                        24/24 - 0s - loss: 1.4038 - accuracy: 0.4600<br>
                        Epoch 42/120<br>
                        24/24 - 0s - loss: 1.4056 - accuracy: 0.4733<br>
                        Epoch 43/120<br>
                        24/24 - 0s - loss: 1.4125 - accuracy: 0.4587<br>
                        Epoch 44/120<br>
                        24/24 - 0s - loss: 1.3902 - accuracy: 0.4707<br>
                        Epoch 45/120<br>
                        24/24 - 0s - loss: 1.3889 - accuracy: 0.4707<br>
                        Epoch 46/120<br>
                        24/24 - 0s - loss: 1.3858 - accuracy: 0.4667<br>
                        Epoch 47/120<br>
                        24/24 - 0s - loss: 1.3715 - accuracy: 0.4653<br>
                        Epoch 48/120<br>
                        24/24 - 0s - loss: 1.3684 - accuracy: 0.4653<br>
                        Epoch 49/120<br>
                        24/24 - 0s - loss: 1.3825 - accuracy: 0.4760<br>
                        Epoch 50/120<br>
                        24/24 - 0s - loss: 1.3719 - accuracy: 0.4827<br>
                        Epoch 51/120<br>
                        24/24 - 0s - loss: 1.3806 - accuracy: 0.4520<br>
                        Epoch 52/120<br>
                        24/24 - 0s - loss: 1.3710 - accuracy: 0.4867<br>
                        Epoch 53/120<br>
                        24/24 - 0s - loss: 1.3847 - accuracy: 0.4627<br>
                        Epoch 54/120<br>
                        24/24 - 0s - loss: 1.3906 - accuracy: 0.4680<br>
                        Epoch 55/120<br>
                        24/24 - 0s - loss: 1.4286 - accuracy: 0.4507<br>
                        Epoch 56/120<br>
                        24/24 - 0s - loss: 1.3688 - accuracy: 0.4733<br>
                        Epoch 57/120<br>
                        24/24 - 0s - loss: 1.3696 - accuracy: 0.4480<br>
                        Epoch 58/120<br>
                        24/24 - 0s - loss: 1.3585 - accuracy: 0.4707<br>
                        Epoch 59/120<br>
                        24/24 - 0s - loss: 1.3462 - accuracy: 0.4853<br>
                        Epoch 60/120<br>
                        24/24 - 0s - loss: 1.3428 - accuracy: 0.4827<br>
                        Epoch 61/120<br>
                        24/24 - 0s - loss: 1.3644 - accuracy: 0.4733<br>
                        Epoch 62/120<br>
                        24/24 - 0s - loss: 1.3525 - accuracy: 0.4813<br>
                        Epoch 63/120<br>
                        24/24 - 0s - loss: 1.3627 - accuracy: 0.4707<br>
                        Epoch 64/120<br>
                        24/24 - 0s - loss: 1.3659 - accuracy: 0.4693<br>
                        Epoch 65/120<br>
                        24/24 - 0s - loss: 1.3467 - accuracy: 0.4747<br>
                        Epoch 66/120<br>
                        24/24 - 0s - loss: 1.3372 - accuracy: 0.4947<br>
                        Epoch 67/120<br>
                        24/24 - 0s - loss: 1.3216 - accuracy: 0.4960<br>
                        Epoch 68/120<br>
                        24/24 - 0s - loss: 1.3313 - accuracy: 0.4933<br>
                        Epoch 69/120<br>
                        24/24 - 0s - loss: 1.3450 - accuracy: 0.4880<br>
                        Epoch 70/120
                        24/24 - 0s - loss: 1.3244 - accuracy: 0.4920<br>
                        Epoch 71/120<br>
                        24/24 - 0s - loss: 1.3246 - accuracy: 0.4867<br>
                        Epoch 72/120<br>
                        24/24 - 0s - loss: 1.3434 - accuracy: 0.4840<br>
                        Epoch 73/120<br>
                        24/24 - 0s - loss: 1.3372 - accuracy: 0.4880<br>
                        Epoch 74/120<br>
                        24/24 - 0s - loss: 1.3632 - accuracy: 0.4800<br>
                        Epoch 75/120<br>
                        24/24 - 0s - loss: 1.3254 - accuracy: 0.4867<br>
                        Epoch 76/120<br>
                        24/24 - 0s - loss: 1.3177 - accuracy: 0.4947<br>
                        Epoch 77/120<br>
                        24/24 - 0s - loss: 1.3359 - accuracy: 0.4787<br>
                        Epoch 78/120<br>
                        24/24 - 0s - loss: 1.3456 - accuracy: 0.4840<br>
                        Epoch 79/120<br>
                        24/24 - 0s - loss: 1.3042 - accuracy: 0.5040<br>
                        Epoch 80/120<br>
                        24/24 - 0s - loss: 1.3063 - accuracy: 0.5000<br>
                        Epoch 81/120<br>
                        24/24 - 0s - loss: 1.3009 - accuracy: 0.5000<br>
                        Epoch 82/120<br>
                        24/24 - 0s - loss: 1.3182 - accuracy: 0.4880<br>
                        Epoch 83/120<br>
                        24/24 - 0s - loss: 1.3055 - accuracy: 0.5013<br>
                        Epoch 84/120<br>
                        24/24 - 0s - loss: 1.3048 - accuracy: 0.4960<br>
                        Epoch 85/120<br>
                        24/24 - 0s - loss: 1.2847 - accuracy: 0.5000<br>
                        Epoch 86/120<br>
                        24/24 - 0s - loss: 1.2839 - accuracy: 0.5040<br>
                        Epoch 87/120<br>
                        24/24 - 0s - loss: 1.2826 - accuracy: 0.5107<br>
                        Epoch 88/120<br>
                        24/24 - 0s - loss: 1.3180 - accuracy: 0.5133<br>
                        Epoch 89/120<br>
                        24/24 - 0s - loss: 1.2846 - accuracy: 0.4973<br>
                        Epoch 90/120<br>
                        24/24 - 0s - loss: 1.2763 - accuracy: 0.5133<br>
                        Epoch 91/120<br>
                        24/24 - 0s - loss: 1.2915 - accuracy: 0.4960<br>
                        Epoch 92/120<br>
                        24/24 - 0s - loss: 1.2936 - accuracy: 0.5147<br>
                        Epoch 93/120<br>
                        24/24 - 0s - loss: 1.3172 - accuracy: 0.4787<br>
                        Epoch 94/120<br>
                        24/24 - 0s - loss: 1.3127 - accuracy: 0.4880<br>
                        Epoch 95/120<br>
                        24/24 - 0s - loss: 1.2843 - accuracy: 0.5053<br>
                        Epoch 96/120<br>
                        24/24 - 0s - loss: 1.2833 - accuracy: 0.5053<br>
                        Epoch 97/120<br>
                        24/24 - 0s - loss: 1.2981 - accuracy: 0.5147<br>
                        Epoch 98/120<br>
                        24/24 - 0s - loss: 1.2903 - accuracy: 0.5067<br>
                        Epoch 99/120<br>
                        24/24 - 0s - loss: 1.2836 - accuracy: 0.5040<br>
                        Epoch 100/120<br>
                        24/24 - 0s - loss: 1.2910 - accuracy: 0.5227<br>
                        Epoch 101/120<br>
                        24/24 - 0s - loss: 1.2739 - accuracy: 0.5227<br>
                        Epoch 102/120<br>
                        24/24 - 0s - loss: 1.2665 - accuracy: 0.5173<br>
                        Epoch 103/120<br>
                        24/24 - 0s - loss: 1.2581 - accuracy: 0.5253<br>
                        Epoch 104/120<br>
                        24/24 - 0s - loss: 1.2656 - accuracy: 0.5187<br>
                        Epoch 105/120<br>
                        24/24 - 0s - loss: 1.2652 - accuracy: 0.5120<br>
                        Epoch 106/120<br>
                        24/24 - 0s - loss: 1.2522 - accuracy: 0.5200<br>
                        Epoch 107/120<br>
                        24/24 - 0s - loss: 1.2427 - accuracy: 0.5307<br>
                        Epoch 108/120<br>
                        24/24 - 0s - loss: 1.2527 - accuracy: 0.5227<br>
                        Epoch 109/120<br>
                        24/24 - 0s - loss: 1.2820 - accuracy: 0.5013<br>
                        Epoch 110/120<br>
                        24/24 - 0s - loss: 1.2789 - accuracy: 0.4853<br>
                        Epoch 111/120<br>
                        24/24 - 0s - loss: 1.2435 - accuracy: 0.5187<br>
                        Epoch 112/120<br>
                        24/24 - 0s - loss: 1.2681 - accuracy: 0.5107<br>
                        Epoch 113/120<br>
                        24/24 - 0s - loss: 1.2485 - accuracy: 0.5200<br>
                        Epoch 114/120<br>
                        24/24 - 0s - loss: 1.2335 - accuracy: 0.5160<br>
                        Epoch 115/120<br>
                        24/24 - 0s - loss: 1.2384 - accuracy: 0.5253<br>
                        Epoch 116/120<br>
                        24/24 - 0s - loss: 1.2315 - accuracy: 0.5347<br>
                        Epoch 117/120<br>
                        24/24 - 0s - loss: 1.2176 - accuracy: 0.5240<br>
                        Epoch 118/120<br>
                        24/24 - 0s - loss: 1.2267 - accuracy: 0.5440<br>
                        Epoch 119/120<br>
                        24/24 - 0s - loss: 1.2507 - accuracy: 0.5280<br>
                        Epoch 120/120<br>
                        24/24 - 0s - loss: 1.2329 - accuracy: 0.5280<br>
                        8/8 - 0s - loss: 1.4906 - accuracy: 0.4600<br>
                        Normal Neural Network - Loss: 1.4905989170074463, Accuracy: 0.46000000834465027
                      </div>
                      <b>Note: remember to move the decimal 2-places! <span class="greenText">&#129299;</span></b>
                    </p>
                  </div>
                </li>

                <li>
                  <a data-toggle="collapse" href="#accordion-list3" class="collapsed">3. Quantifying our model (Trained Classifer) <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list3" class="collapse" data-parent=".accordion-list1">
                    <p>
                      Whilst the previous test is the accuracy rating for the training data, the second test is concerned with the accuracy rating for the testing data:<br>
                      <pre>
                        <code class="language-python">
                          model_loss, model_accuracy = model.evaluate(
                              X_test_scaled, y_test_categorical, verbose=2)
                          print(
                          f"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}")
                        </code>
                      </pre>
                      <br> <span class="greenText">&#128221;</span> <b>Results: </b> Here are the testing results of the model, against testing data: <br><br>
                      <div style="height:60px;width:765px;border:1px solid #ccc;font:16px/26px Georgia, Garamond, Serif;overflow:auto;">
                        8/8 - 0s - loss: 1.4906 - accuracy: 0.4600<br>
                        Normal Neural Network - Loss: 1.4905989170074463, Accuracy: 0.46000000834465027
                      </div>
                      <b>Note: remember to move the decimal 2-places! <span class="greenText">&#129299;</span></b>
                    </p>
                  </div>
                </li>
                <li>
                  <a data-toggle="collapse" href="#accordion-list4" class="collapsed">4. Making Predictions <i class="bx bx-chevron-down icon-show"></i><i class="bx bx-chevron-up icon-close"></i></a>
                  <div id="accordion-list4" class="collapse" data-parent=".accordion-list1">
                    <p>
                      In this final phase, our now Trained Classifer model would be put through it's paces in terms of efficiency and indeed accuracy:<br>
                      <br> <span class="greenText">(&#10004;)</span> The first testing stage involved putting the machines predictions and test values into variables - <br>
                      <pre>
                        <code class="language-python">
                          encoded_predictions = model.predict_classes(X_test_scaled[:10])
                          prediction_labels = label_encoder.inverse_transform(encoded_predictions)
                        </code>
                      </pre>
                      <br> <span class="greenText">(&#10004;)</span> Before then printing the machine predictions and test values to draw comparison - <br>
                      <pre>
                        <code class="language-python">
                          print(f"Predicted classes: {prediction_labels}")
                          print(f"Actual Labels: {list(y_test[:10])}")
                        </code>
                      </pre>
                      <div style="height:60px;width:765px;border:1px solid #ccc;font:16px/26px Georgia, Garamond, Serif;overflow:auto;">
                        <b>Predicted classes:</b> ['hiphop' 'reggae' 'h_metal' 'rock' 'country' 'rock' 'rock' 'reggae' 'pop'
                         'blues']<br>
                        <b>Actual Labels:</b> ['hiphop', 'reggae', 'h_metal', 'disco', 'country', 'rock', 'country', 'reggae', 'disco', 'blues']
                      </div>
                      <br><span class="greenText">(&#10004;)</span>The final step was now to print the info / stats of the model for review - <br>
                      <pre>
                        <code class="language-python">
                          model_loss, model_accuracy = genre_model.evaluate(
                              X_test_scaled, y_test_categorical, verbose=2)
                          print(
                              f"Music Genre Classification:Deep Learning Model - Loss: {model_loss}, Accuracy: {model_accuracy}")
                        </code>
                      </pre>
                      <div style="height:60px;width:765px;border:1px solid #ccc;font:16px/26px Georgia, Garamond, Serif;overflow:auto;">
                        <b>8/8 - 0s - loss:</b> 1.5351 - accuracy: 0.5080<br>
                        <b>Music Genre Classification:Deep Learning Model - Loss:</b> 1.5351499319076538, <b>Accuracy:</b> 0.5080000162124634
                    </p>
                  </div>
                </li>

              </ul>
            </div>

          </div>

          <div class="col-lg-5 align-items-stretch order-1 order-lg-2 img" style='background-image: url("assets/img/machine_learning_code_image.png");'>&nbsp;</div>
        </div>

      </div>
    </section><!-- End Data Generation with Librosa Section Section -->

    <!-- ======= Cta Section ======= -->
    <section id="cta3" class="cta3">
          <div class="container">

            <div class="row">
              <div class="col-lg-9 text-center text-lg-left">
                <h3>Tensorflow Fact Check</h3>
                <p> <code class="language-python">TensorFlow</code> is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.
</p>
              </div>
              <div class="col-lg-3 cta-btn-container text-center">
                <a class="cta-btn align-middle" href="https://www.tensorflow.org">Visit Tensorflow</a>
              </div>
            </div>
          </div>
    </section>
    <!-- End Cta Section -->

    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact">
      <div class="container">

        <div class="section-title">
          <h2>The Team</h2>
          <p><b>Thank you...</b>for taking the time to come on a journey of Machine Learning discovery with us.
            This project is the culmination of a 6-month intensive Data Analytics Boot Camp, run by U.C Davis.<br><br>If
          you have any questions or feedback on this project, we'd love to here from you...</p>
        </div>

        <div class="row">

          <div class="col-lg-4 col-md-6">
            <div class="box">
              <img src="assets/img/team/william.png">
              <h3>William Forsyth</h3>
              <div class="social">
            <a class="social-icon" data-tooltip="williamtcforsyth@icloud.com" href="mailto:williamtcforsysth@icloud.com">
              <i class="fa fa-envelope" aria-hidden="true"></i>
            </a>

            <a class="social-icon" data-tooltip="SaltireSequence" href="https://github.com/saltiresequence", target="blank">
              <i class="fa fa-github" aria-hidden="true"></i>
            </a>

            <a class="social-icon" data-tooltip="williamtcforsyth" href="https://www.linkedin.com/in/williamtcforsyth", target="blank">
              <i class="fa fa-linkedin" aria-hidden="true"></i>
            </a>
          </div>
              <div class="btn-wrap">
                <a href="mailto:williamtcforsyth@icloud.com" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-4 mt-lg-0">
            <div class="box">
              <img src="assets/img/team/danne.png">
              <h3>Danne Paredes</h3>
              <a class="social-icon" data-tooltip="email@gmail.com" href="#">
                <i class="fa fa-envelope" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="HotAsPho" href="https://github.com/HotAsPho", target="blank">
                <i class="fa fa-github" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="danne-paredes-70bb4756" href="https://www.linkedin.com/in/danne-paredes-70bb4756/", target="blank">
                <i class="fa fa-linkedin" aria-hidden="true"></i>
              </a>
              <div class="btn-wrap">
                <a href="#" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-4 mt-lg-0">
            <div class="box">
              <img src="assets/img/team/heesung.png">
              <h3>Heesung Shim</h3>
              <a class="social-icon" data-tooltip="hsshim@ucdavis.edu" href="mailto:hsshim@ucdavis.edu">
                <i class="fa fa-envelope" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="Heesung80" href="https://github.com/Heesung80", target="blank">
                <i class="fa fa-github" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="heesung-shim-a0a34733" href="https://www.linkedin.com/in/heesung-shim-a0a34733/", target="blank">
                <i class="fa fa-linkedin" aria-hidden="true"></i>
              </a>
              <div class="btn-wrap">
                <a href="mailto:hsshim@ucdavis.edu" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-4 mt-lg-4">
            <div class="box">
              <img src="assets/img/team/caitlin.png">
              <h3>Caitlin Calsbeek</h3>
              <a class="social-icon" data-tooltip="caitlin.calsbeek@gmail.com" href="mailto:caitlin.calsbeek@gmail.com">
                <i class="fa fa-envelope" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="Ccalsbeek" href="https://github.com/ccalsbeek", target="blank">
                <i class="fa fa-github" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="caitlin-calsbeek-93727a1a4" href="https://www.linkedin.com/in/caitlin-calsbeek-93727a1a4/" target="blank">
                <i class="fa fa-linkedin" aria-hidden="true"></i>
              </a>
              <div class="btn-wrap">
                <a href="mailto:caitlin.calsbeek@gmail.com" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-4 mt-lg-4">
            <div class="box">
              <img src="assets/img/team/kathryn.png">
              <h3>Kathryn Panger</h3>
              <a class="social-icon" data-tooltip="kpanger88@gmail.com" href="mailto:kpanger88@gmail.com">
                <i class="fa fa-envelope" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="KathrynPanger" href="https://github.com/KathrynPanger", target="blank">
                <i class="fa fa-github" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="kathryn-panger" href="https://www.linkedin.com/in/kathryn-panger/", target="blank">
                <i class="fa fa-linkedin" aria-hidden="true"></i>
              </a>
              <div class="btn-wrap">
                <a href="mailto:kpanger88@gmail.com" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>

          <div class="col-lg-4 col-md-6 mt-4 mt-lg-4">
            <div class="box">
              <img src="assets/img/team/seth.png">
              <h3>Seth Abbott</h3>
              <a class="social-icon" data-tooltip="sacstateseth@gmail.com" href="mailto:sacstateseth@gmail.com">
                <i class="fa fa-envelope" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="SethAbott916" href="https://github.com/SethAbbott916", target="blank">
                <i class="fa fa-github" aria-hidden="true"></i>
              </a>

              <a class="social-icon" data-tooltip="seth-abbott-68789a7" href="https://www.linkedin.com/in/seth-abbott-68789a71/", target="blank">
                <i class="fa fa-linkedin" aria-hidden="true"></i>
              </a>
              <div class="btn-wrap">
                <a href="mailto:sacstateseth@gmail.com" class="btn-buy">Contact Me</a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section><!-- End Pricing Section -->


  </main><!-- End #main -->
  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <h3>Machine Learning: Genre Classification</h3>
      <div class="social-links">
        <a href="https://github.com/KathrynPanger/classify-spotify", target="blank", class="github"><i class="bx bxl-github"></i></a>
      </div>
      <div class="copyright">
        &copy; Copyright <strong><span>William Forsyth, Caitlin Calsbeek, Kathryn Panger,<br>Danne Paredes, Heesung Shim, Seth Abbott<br></span></strong> All Rights Reserved
      </div>

    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top"><i class="ri-arrow-up-line"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="assets/vendor/counterup/counterup.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/js/prism.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
